---
title: Predicting hotspots
author: Justin, Natalia 
date: Aug 26, 2020
output:
  html_document:
    code_folding: show
    toc: true
    toc_float: true
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=14, fig.height=8, echo=TRUE, eval=FALSE, cache=TRUE,
                      warning=FALSE, message=FALSE)
library(ggplot2)
library(covidcast)
library(tidyverse)
library(tidyr) ## for pivot_wider() ?
library(devtools)
library(glmnet)
source("helpers.r")

## Location of your covidcast R package.
load_all("/home/shyun/repos/covidcast/R-packages/covidcast")
## load_all("~/Desktop/CMU/Projects/Delphi-Covid-19/delphi_repos/covidcast/R-packages/covidcast")
```

## Goal

Use various classifiers (e.g. l1-penalized and ridge logistic regression, and
SVM, xgboost), for a hotspot detection model.

## Data and model 

1. **Data** Between 2020-05-01 and 2020-08-10, take the 7-day case proportion,
   and various FB surveys (smoothed individual, smoothed community survey,
   smoothed household survey, smoothed weighted household survey). Also, add
   features that are "slopes from the past x days", where $x \in
   \{3,5,10,15,20..\}$. Covariates are formed using lagged FB survey values and
   case proportion.

2. **Response data** 
  - Basic version is: (1 if 25% increase in the next K weeks compared to the
    past 1 week; 0 otherwise);
  - Other options: Some minimum threshold for incidence before deeming hotspot;
    Redefine hotspot as onset -- by stopping the definition of hotspot if it has
    already occurred many times in the past.

3. **Validation data** are some left-out geo levels (or times). **This hasn't
   been used yet**

4. **Training and test data** are the rest of the geo levels (or times); these
    are again split into training (e.g. for cross-validation) and testing data
    (used only for creating the ROC or adjusted ROC curves).

5. **Models** L1- and L2- penalized logistic regression, SVM, xgboost, and more.

6. **Cross-validation** is done in a block-wise fashion; consecutive time blocks
   are used as test sets.

## Code

Some variables and functions:

- `lags` are the number of lagged values taken of the JHU cases and FB.
- `n_ahead` is the number of days we want to look ahead, in forming the response
  data (0 or 1) for hotspot detection. 
- We compare the $\bar y_2$ average of 1(?) week period ending `n_ahead` days
  from *now*, to the average $\bar y_1$ of the past 1 week period (again, from
  *now*).
- We deem one time & region a "hotspot" if $\bar y_1$ is larger than $\bar y_2$
  by `threshold` percent (say, 25%).
- `covidcast_signals()` from the R package `covidcast` allows the data
  collection of multiple sources.
- `read_to_model()` further cleans + readies the data to feed into functions
  like `glmnet()`.

First, we load the data.

```{r read-data}
## Setup
lags = 28
geo_type = "state" ## county, MSA
## geo_type = "county"
response = "confirmed_7dav_incidence_prop"
fn_response = response_diff_avg_1week_min30
fn_response_name = "response_diff_avg_1week"

## Read in data once
data_sources = c("indicator-combination", 
                 "fb-survey", 
                 "fb-survey", 
                 "fb-survey", 
                 "fb-survey")
signals = c("confirmed_7dav_incidence_prop", 
            "smoothed_cli", 
            "smoothed_nohh_cmnty_cli", 
            "smoothed_wcli", 
            "smoothed_hh_cmnty_cli")
start_day = as.Date("2020-05-01")
end_day = as.Date("2020-08-10")
validation_days = seq(end_day-30, end_day, by = "days") ## Old! to be retired soon

signals = data.frame(data_sources = data_sources, signals = signals)
suppressMessages({
  mat = covidcast_signals(signals,
                          start_day = start_day, end_day = end_day, geo_type = geo_type)
})
mat <- mat %>% select(geo_value, time_value, signal, data_source, value) 

## New: instead of validation_days, use validation_geos
geos = mat %>% select(geo_value) %>% unlist() %>% unique()
set.seed(1000)
pct_test = 0.3
validation_ind = sample(length(geos), length(geos) * pct_test)
validation_geos = geos[validation_ind]
```

Now, fit the hot-spot models and visualize the model performances.

```{r analyze}
## ## Try all configurations
## configs = expand.grid(n_ahead = c(28,21,14),
##                       threshold = c(.25, .40),
##                       split_type = c("time", "geo"),
##                       slope = c(TRUE, FALSE))
##                       ## onset = c(TRUE, FALSE))

## mclapply(1:nrow(configs), function(irow){

##   ## Default configuration
##   onset = FALSE

##   ## Load configuration
##   configs[irow,] %>% list2env(globalenv())
##   cat(n_ahead, threshold, split_type, slope, onset, fill=TRUE) ## check that it's working
##   })


  ## Picking one setting: feel free to change
  slope = onset = FALSE 
  n_ahead = 28
  threshold = .25
  split_type = "time"

  ## Make a y|X model matrix ready to model
  df_model <- ready_to_model(mat, lags, n_ahead, response, slope, fn_response, threshold, onset)

  ## Split by time or geo
  if(split_type == "time"){
    df_traintest <- df_model %>% filter(!(time_value %in% validation_days))
    df_validation <- df_model %>% filter(time_value %in% validation_days)
    splitted <- sample_split_date(df_traintest, pct_test = 0.3)
  } else {
    df_traintest <- df_model %>% filter(!(geo_value %in% validation_geos))
    df_validation <- df_model %>% filter(geo_value %in% validation_geos)
    splitted <- sample_split_geo(df_traintest, pct_test = 0.3)

    ## Temporary check: show how many ones
    df_model %>% select(resp) %>% table()
    df_traintest %>% select(resp) %>% table()
    df_validation %>% select(resp) %>% table()
  }
  make_plots(destin = "figures", splitted, lags, n_ahead, geo_type, fn_response_name, threshold, slope,
             split_type, onset)
## }, mc.cores = 12)

```

## Visualizing the response

We can also visualize the response, varying a few things: `fn_response`,
`onset`, `threshold`, `n_ahead` etc.

(FYI lots of duplicate code here)

```{r}
###########
## Setup ##
###########
lags = 28
geo_type = "state"
## geo_type = "county"
response = "confirmed_7dav_incidence_prop"
fn_response = response_diff_avg_1week_min20 
## fn_response = response_diff_avg_1week 
fn_response_name = "response_diff_avg_1week_min20"
slope = FALSE
## slope = FALSE
onset = FALSE
n_ahead = 28
threshold = .25

## Read in data once
data_sources = c("indicator-combination", 
                 "fb-survey", 
                 "fb-survey", 
                 "fb-survey", 
                 "fb-survey")
signals = c("confirmed_7dav_incidence_prop", 
            "smoothed_cli", 
            "smoothed_nohh_cmnty_cli", 
            "smoothed_wcli", 
            "smoothed_hh_cmnty_cli")
start_day = as.Date("2020-05-01")
end_day = as.Date("2020-08-25")
validation_days = seq(end_day-30, end_day, by = "days") ## Old! to be retired soon

signals = data.frame(data_sources = data_sources, signals = signals)
suppressMessages({
  mat = covidcast_signals(signals,
                          start_day = start_day, end_day = end_day, geo_type = geo_type)
})
mat <- mat %>% select(geo_value, time_value, signal, data_source, value) 
df_model <- ready_to_model(mat, lags, n_ahead, response, slope, fn_response, threshold, onset)
df_validation <- df_model %>% filter(time_value %in% validation_days)
df_traintest <- df_model %>% filter(!(time_value %in% validation_days))
splitted <- sample_split_date(df_traintest, pct_test = 0.3)

## New: instead of validation_days, use validation_geos
geos = mat %>% select(geo_value) %>% unlist() %>% unique()
set.seed(1000)
pct_test = 0.3
validation_ind = sample(length(geos), length(geos) * pct_test)
validation_geos = geos[validation_ind]

df_validation <- df_model %>% filter(geo_value %in% validation_geos)
df_traintest <- df_model %>% filter(!(geo_value %in% validation_geos))
splitted <- sample_split_date(df_traintest, pct_test = 0.3)

## Seeing the proportion of 0's and 1's in the response.
df_validation %>% select(resp) %>% unlist() %>% table() %>% print()
df_model %>% select(resp) %>% unlist() %>% table()

## Visualize the response with the data.
geos = df_model %>% select(geo_value) %>% unlist() %>% unique()
nn1 = 7
nn2 = 8
par(mfrow = c(nn1, nn2)); par(mar = c(3, 3, 1, 1)); par(cex=.7)
iilist = 1:length(geos)
for(ii in iilist){

  ## Form our response data (0's and 1's)
  dat0 = df_model %>% subset(geo_value==geos[ii]) %>%
  select(time_value,
         resp, 
         incidence = "feature_lag0_confirmed_7dav_incidence_prop_indicator-combination" ) 

  ## Combine it with original data for this geo
  dat = mat%>% as_tibble() %>% filter(geo_value==geos[ii], signal =="confirmed_7dav_incidence_prop") %>%
    select(time_value, incidence = value) 
  dat_combined = dat %>% full_join(dat0, by = c("time_value", "incidence"))
  dat_combined = dat_combined %>% mutate(resp=replace(resp, is.na(resp), 0))

  ## Make a plot
  dat_combined %>% with(plot(time_value, incidence, col = resp+1, type='o', pch=16, ylim= c(0,100)))
  abline(h = 30, col='grey50', lwd=3)
  legend("topleft", legend=geos[ii], bty='n', cex=3)
}
```
